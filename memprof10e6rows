 
/data/home/jli819/Temp/CichlidAnalyzer/MC9_1/SubAnalysis/0007_vid/0007_vid.hmm.npy
True
Hmmfile already exists. Will not recalculate it unless rewrite flag is True
already exists filteredCoord, not rewriting
self.coords size: 800112
self.coords count: 100000
dist size: 105734944
not deleting
Filename: /data/home/jli819/.local/lib/python3.6/site-packages/numpy/lib/npyio.py

Line #    Mem usage    Increment   Line Contents
================================================
   526 1208.844 MiB 1208.844 MiB   @profile
   527                             def savez(file, *args, **kwds):
   528                                 """
   529                                 Save several arrays into a single file in uncompressed ``.npz`` format.
   530                             
   531                                 If arguments are passed in with no keywords, the corresponding variable
   532                                 names, in the ``.npz`` file, are 'arr_0', 'arr_1', etc. If keyword
   533                                 arguments are given, the corresponding variable names, in the ``.npz``
   534                                 file will match the keyword names.
   535                             
   536                                 Parameters
   537                                 ----------
   538                                 file : str or file
   539                                     Either the file name (string) or an open file (file-like object)
   540                                     where the data will be saved. If file is a string or a Path, the
   541                                     ``.npz`` extension will be appended to the file name if it is not
   542                                     already there.
   543                                 args : Arguments, optional
   544                                     Arrays to save to the file. Since it is not possible for Python to
   545                                     know the names of the arrays outside `savez`, the arrays will be saved
   546                                     with names "arr_0", "arr_1", and so on. These arguments can be any
   547                                     expression.
   548                                 kwds : Keyword arguments, optional
   549                                     Arrays to save to the file. Arrays will be saved in the file with the
   550                                     keyword names.
   551                             
   552                                 Returns
   553                                 -------
   554                                 None
   555                             
   556                                 See Also
   557                                 --------
   558                                 save : Save a single array to a binary file in NumPy format.
   559                                 savetxt : Save an array to a file as plain text.
   560                                 savez_compressed : Save several arrays into a compressed ``.npz`` archive
   561                             
   562                                 Notes
   563                                 -----
   564                                 The ``.npz`` file format is a zipped archive of files named after the
   565                                 variables they contain.  The archive is not compressed and each file
   566                                 in the archive contains one variable in ``.npy`` format. For a
   567                                 description of the ``.npy`` format, see :py:mod:`numpy.lib.format`.
   568                             
   569                                 When opening the saved ``.npz`` file with `load` a `NpzFile` object is
   570                                 returned. This is a dictionary-like object which can be queried for
   571                                 its list of arrays (with the ``.files`` attribute), and for the arrays
   572                                 themselves.
   573                             
   574                                 Examples
   575                                 --------
   576                                 >>> from tempfile import TemporaryFile
   577                                 >>> outfile = TemporaryFile()
   578                                 >>> x = np.arange(10)
   579                                 >>> y = np.sin(x)
   580                             
   581                                 Using `savez` with \\*args, the arrays are saved with default names.
   582                             
   583                                 >>> np.savez(outfile, x, y)
   584                                 >>> outfile.seek(0) # Only needed here to simulate closing & reopening file
   585                                 >>> npzfile = np.load(outfile)
   586                                 >>> npzfile.files
   587                                 ['arr_1', 'arr_0']
   588                                 >>> npzfile['arr_0']
   589                                 array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
   590                             
   591                                 Using `savez` with \\**kwds, the arrays are saved with the keyword names.
   592                             
   593                                 >>> outfile = TemporaryFile()
   594                                 >>> np.savez(outfile, x=x, y=y)
   595                                 >>> outfile.seek(0)
   596                                 >>> npzfile = np.load(outfile)
   597                                 >>> npzfile.files
   598                                 ['y', 'x']
   599                                 >>> npzfile['x']
   600                                 array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
   601                             
   602                                 """
   603 1222.191 MiB 1222.191 MiB       _savez(file, args, kwds, False)


Filename: /data/home/jli819/.local/lib/python3.6/site-packages/numpy/lib/npyio.py

Line #    Mem usage    Increment   Line Contents
================================================
   669 1208.844 MiB 1208.844 MiB   @profile
   670                             def _savez(file, args, kwds, compress, allow_pickle=True, pickle_kwargs=None):
   671                                 # Import is postponed to here since zipfile depends on gzip, an optional
   672                                 # component of the so-called standard library.
   673 1208.844 MiB    0.000 MiB       import zipfile
   674                             
   675 1208.844 MiB    0.000 MiB       if isinstance(file, basestring):
   676 1208.844 MiB    0.000 MiB           if not file.endswith('.npz'):
   677                                         file = file + '.npz'
   678                                 elif is_pathlib_path(file):
   679                                     if not file.name.endswith('.npz'):
   680                                         file = file.parent / (file.name + '.npz')
   681                             
   682 1208.844 MiB    0.000 MiB       namedict = kwds
   683 1208.844 MiB    0.000 MiB       for i, val in enumerate(args):
   684                                     key = 'arr_%d' % i
   685                                     if key in namedict.keys():
   686                                         raise ValueError(
   687                                             "Cannot use un-named variables and keyword %s" % key)
   688                                     namedict[key] = val
   689                             
   690 1208.844 MiB    0.000 MiB       if compress:
   691                                     compression = zipfile.ZIP_DEFLATED
   692                                 else:
   693 1208.844 MiB    0.000 MiB           compression = zipfile.ZIP_STORED
   694                             
   695 1208.844 MiB    0.000 MiB       zipf = zipfile_factory(file, mode="w", compression=compression)
   696                             
   697 1208.844 MiB    0.000 MiB       if sys.version_info >= (3, 6):
   698                                     # Since Python 3.6 it is possible to write directly to a ZIP file.
   699 1222.191 MiB    0.000 MiB           for key, val in namedict.items():
   700 1222.188 MiB    0.000 MiB               fname = key + '.npy'
   701 1222.188 MiB    0.000 MiB               val = np.asanyarray(val)
   702 1222.188 MiB    0.000 MiB               force_zip64 = val.nbytes >= 2**30
   703 1222.188 MiB    0.000 MiB               with zipf.open(fname, 'w', force_zip64=force_zip64) as fid:
   704 1222.188 MiB    0.000 MiB                   format.write_array(fid, val,
   705 1222.188 MiB    0.000 MiB                                      allow_pickle=allow_pickle,
   706 1222.191 MiB   13.348 MiB                                      pickle_kwargs=pickle_kwargs)
   707                                 else:
   708                                     # Stage arrays in a temporary file on disk, before writing to zip.
   709                             
   710                                     # Import deferred for startup time improvement
   711                                     import tempfile
   712                                     # Since target file might be big enough to exceed capacity of a global
   713                                     # temporary directory, create temp file side-by-side with the target file.
   714                                     file_dir, file_prefix = os.path.split(file) if _is_string_like(file) else (None, 'tmp')
   715                                     fd, tmpfile = tempfile.mkstemp(prefix=file_prefix, dir=file_dir, suffix='-numpy.npy')
   716                                     os.close(fd)
   717                                     try:
   718                                         for key, val in namedict.items():
   719                                             fname = key + '.npy'
   720                                             fid = open(tmpfile, 'wb')
   721                                             try:
   722                                                 format.write_array(fid, np.asanyarray(val),
   723                                                                    allow_pickle=allow_pickle,
   724                                                                    pickle_kwargs=pickle_kwargs)
   725                                                 fid.close()
   726                                                 fid = None
   727                                                 zipf.write(tmpfile, arcname=fname)
   728                                             except IOError as exc:
   729                                                 raise IOError("Failed to write to %s: %s" % (tmpfile, exc))
   730                                             finally:
   731                                                 if fid:
   732                                                     fid.close()
   733                                     finally:
   734                                         os.remove(tmpfile)
   735                             
   736 1222.191 MiB    0.000 MiB       zipf.close()


Filename: /data/home/jli819/.local/lib/python3.6/site-packages/scipy/sparse/_matrix_io.py

Line #    Mem usage    Increment   Line Contents
================================================
    18 1208.844 MiB 1208.844 MiB   @profile
    19                             def save_npz(file, matrix, compressed=True):
    20                                 """ Save a sparse matrix to a file using ``.npz`` format.
    21                             
    22                                 Parameters
    23                                 ----------
    24                                 file : str or file-like object
    25                                     Either the file name (string) or an open file (file-like object)
    26                                     where the data will be saved. If file is a string, the ``.npz``
    27                                     extension will be appended to the file name if it is not already
    28                                     there.
    29                                 matrix: spmatrix (format: ``csc``, ``csr``, ``bsr``, ``dia`` or coo``)
    30                                     The sparse matrix to save.
    31                                 compressed : bool, optional
    32                                     Allow compressing the file. Default: True
    33                             
    34                                 See Also
    35                                 --------
    36                                 scipy.sparse.load_npz: Load a sparse matrix from a file using ``.npz`` format.
    37                                 numpy.savez: Save several arrays into a ``.npz`` archive.
    38                                 numpy.savez_compressed : Save several arrays into a compressed ``.npz`` archive.
    39                             
    40                                 Examples
    41                                 --------
    42                                 Store sparse matrix to disk, and load it again:
    43                             
    44                                 >>> import scipy.sparse
    45                                 >>> sparse_matrix = scipy.sparse.csc_matrix(np.array([[0, 0, 3], [4, 0, 0]]))
    46                                 >>> sparse_matrix
    47                                 <2x3 sparse matrix of type '<class 'numpy.int64'>'
    48                                    with 2 stored elements in Compressed Sparse Column format>
    49                                 >>> sparse_matrix.todense()
    50                                 matrix([[0, 0, 3],
    51                                         [4, 0, 0]], dtype=int64)
    52                             
    53                                 >>> scipy.sparse.save_npz('/tmp/sparse_matrix.npz', sparse_matrix)
    54                                 >>> sparse_matrix = scipy.sparse.load_npz('/tmp/sparse_matrix.npz')
    55                             
    56                                 >>> sparse_matrix
    57                                 <2x3 sparse matrix of type '<class 'numpy.int64'>'
    58                                    with 2 stored elements in Compressed Sparse Column format>
    59                                 >>> sparse_matrix.todense()
    60                                 matrix([[0, 0, 3],
    61                                         [4, 0, 0]], dtype=int64)
    62                                 """
    63 1208.844 MiB    0.000 MiB       arrays_dict = {}
    64 1208.844 MiB    0.000 MiB       if matrix.format in ('csc', 'csr', 'bsr'):
    65 1208.844 MiB    0.000 MiB           arrays_dict.update(indices=matrix.indices, indptr=matrix.indptr)
    66                                 elif matrix.format == 'dia':
    67                                     arrays_dict.update(offsets=matrix.offsets)
    68                                 elif matrix.format == 'coo':
    69                                     arrays_dict.update(row=matrix.row, col=matrix.col)
    70                                 else:
    71                                     raise NotImplementedError('Save is not implemented for sparse matrix of format {}.'.format(matrix.format))
    72 1208.844 MiB    0.000 MiB       arrays_dict.update(
    73 1208.844 MiB    0.000 MiB           format=matrix.format.encode('ascii'),
    74 1208.844 MiB    0.000 MiB           shape=matrix.shape,
    75 1208.844 MiB    0.000 MiB           data=matrix.data
    76                                 )
    77 1208.844 MiB    0.000 MiB       if compressed:
    78                                     np.savez_compressed(file, **arrays_dict)
    79                                 else:
    80 1222.191 MiB 1222.191 MiB           np.savez(file, **arrays_dict)


Filename: /data/home/jli819/.local/lib/python3.6/site-packages/sklearn/cluster/dbscan_.py

Line #    Mem usage    Increment   Line Contents
================================================
    21 1222.191 MiB 1222.191 MiB   @profile
    22                             def dbscan(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None,
    23                                        algorithm='auto', leaf_size=30, p=2, sample_weight=None,
    24                                        n_jobs=None):
    25                                 """Perform DBSCAN clustering from vector array or distance matrix.
    26                             
    27                                 Read more in the :ref:`User Guide <dbscan>`.
    28                             
    29                                 Parameters
    30                                 ----------
    31                                 X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \
    32                                         array of shape (n_samples, n_samples)
    33                                     A feature array, or array of distances between samples if
    34                                     ``metric='precomputed'``.
    35                             
    36                                 eps : float, optional
    37                                     The maximum distance between two samples for them to be considered
    38                                     as in the same neighborhood.
    39                             
    40                                 min_samples : int, optional
    41                                     The number of samples (or total weight) in a neighborhood for a point
    42                                     to be considered as a core point. This includes the point itself.
    43                             
    44                                 metric : string, or callable
    45                                     The metric to use when calculating distance between instances in a
    46                                     feature array. If metric is a string or callable, it must be one of
    47                                     the options allowed by :func:`sklearn.metrics.pairwise_distances` for
    48                                     its metric parameter.
    49                                     If metric is "precomputed", X is assumed to be a distance matrix and
    50                                     must be square. X may be a sparse matrix, in which case only "nonzero"
    51                                     elements may be considered neighbors for DBSCAN.
    52                             
    53                                 metric_params : dict, optional
    54                                     Additional keyword arguments for the metric function.
    55                             
    56                                     .. versionadded:: 0.19
    57                             
    58                                 algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
    59                                     The algorithm to be used by the NearestNeighbors module
    60                                     to compute pointwise distances and find nearest neighbors.
    61                                     See NearestNeighbors module documentation for details.
    62                             
    63                                 leaf_size : int, optional (default = 30)
    64                                     Leaf size passed to BallTree or cKDTree. This can affect the speed
    65                                     of the construction and query, as well as the memory required
    66                                     to store the tree. The optimal value depends
    67                                     on the nature of the problem.
    68                             
    69                                 p : float, optional
    70                                     The power of the Minkowski metric to be used to calculate distance
    71                                     between points.
    72                             
    73                                 sample_weight : array, shape (n_samples,), optional
    74                                     Weight of each sample, such that a sample with a weight of at least
    75                                     ``min_samples`` is by itself a core sample; a sample with negative
    76                                     weight may inhibit its eps-neighbor from being core.
    77                                     Note that weights are absolute, and default to 1.
    78                             
    79                                 n_jobs : int or None, optional (default=None)
    80                                     The number of parallel jobs to run for neighbors search.
    81                                     ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    82                                     ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    83                                     for more details.
    84                             
    85                                 Returns
    86                                 -------
    87                                 core_samples : array [n_core_samples]
    88                                     Indices of core samples.
    89                             
    90                                 labels : array [n_samples]
    91                                     Cluster labels for each point.  Noisy samples are given the label -1.
    92                             
    93                                 See also
    94                                 --------
    95                                 DBSCAN
    96                                     An estimator interface for this clustering algorithm.
    97                             
    98                                 Notes
    99                                 -----
   100                                 For an example, see :ref:`examples/cluster/plot_dbscan.py
   101                                 <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.
   102                             
   103                                 This implementation bulk-computes all neighborhood queries, which increases
   104                                 the memory complexity to O(n.d) where d is the average number of neighbors,
   105                                 while original DBSCAN had memory complexity O(n). It may attract a higher
   106                                 memory complexity when querying these nearest neighborhoods, depending
   107                                 on the ``algorithm``.
   108                             
   109                                 One way to avoid the query complexity is to pre-compute sparse
   110                                 neighborhoods in chunks using
   111                                 :func:`NearestNeighbors.radius_neighbors_graph
   112                                 <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
   113                                 ``mode='distance'``, then using ``metric='precomputed'`` here.
   114                             
   115                                 Another way to reduce memory and computation time is to remove
   116                                 (near-)duplicate points and use ``sample_weight`` instead.
   117                             
   118                                 References
   119                                 ----------
   120                                 Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
   121                                 Algorithm for Discovering Clusters in Large Spatial Databases with Noise".
   122                                 In: Proceedings of the 2nd International Conference on Knowledge Discovery
   123                                 and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
   124                                 """
   125 1222.191 MiB    0.000 MiB       if not eps > 0.0:
   126                                     raise ValueError("eps must be positive.")
   127                             
   128 1222.191 MiB    0.000 MiB       X = check_array(X, accept_sparse='csr')
   129 1222.191 MiB    0.000 MiB       if sample_weight is not None:
   130                                     sample_weight = np.asarray(sample_weight)
   131                                     check_consistent_length(X, sample_weight)
   132                             
   133                                 # Calculate neighborhood for all samples. This leaves the original point
   134                                 # in, which needs to be considered later (i.e. point i is in the
   135                                 # neighborhood of point i. While True, its useless information)
   136 1222.191 MiB    0.000 MiB       if metric == 'precomputed' and sparse.issparse(X):
   137 1222.191 MiB    0.000 MiB           neighborhoods = np.empty(X.shape[0], dtype=object)
   138 1222.195 MiB    0.004 MiB           X.sum_duplicates()  # XXX: modifies X's internals in-place
   139 1222.195 MiB    0.000 MiB           X_mask = X.data <= eps
   140 1302.539 MiB   80.344 MiB           masked_indices = X.indices.astype(np.intp, copy=False)[X_mask]
   141 1302.543 MiB    0.004 MiB           masked_indptr = np.concatenate(([0], np.cumsum(X_mask)))[X.indptr[1:]]
   142                             
   143                                     # insert the diagonal: a point is its own neighbor, but 0 distance
   144                                     # means absence from sparse matrix data
   145 1302.543 MiB    0.000 MiB           masked_indices = np.insert(masked_indices, masked_indptr,
   146 1313.113 MiB   10.570 MiB                                      np.arange(X.shape[0]))
   147 1313.113 MiB    0.000 MiB           masked_indptr = masked_indptr[:-1] + np.arange(1, X.shape[0])
   148                                     # split into rows
   149 1320.984 MiB    7.871 MiB           neighborhoods[:] = np.split(masked_indices, masked_indptr)
   150                                 else:
   151                                     neighbors_model = NearestNeighbors(radius=eps, algorithm=algorithm,
   152                                                                        leaf_size=leaf_size,
   153                                                                        metric=metric,
   154                                                                        metric_params=metric_params, p=p,
   155                                                                        n_jobs=n_jobs)
   156                                     neighbors_model.fit(X)
   157                                     # This has worst case O(n^2) memory complexity
   158                                     neighborhoods = neighbors_model.radius_neighbors(X, eps,
   159                                                                                      return_distance=False)
   160                             
   161 1320.984 MiB    0.000 MiB       if sample_weight is None:
   162 1321.035 MiB    0.031 MiB           n_neighbors = np.array([len(neighbors)
   163 1321.035 MiB    0.020 MiB                                   for neighbors in neighborhoods])
   164                                 else:
   165                                     n_neighbors = np.array([np.sum(sample_weight[neighbors])
   166                                                             for neighbors in neighborhoods])
   167                             
   168                                 # Initially, all samples are noise.
   169 1321.035 MiB    0.000 MiB       labels = np.full(X.shape[0], -1, dtype=np.intp)
   170                             
   171                                 # A list of all core samples found.
   172 1321.039 MiB    0.004 MiB       core_samples = np.asarray(n_neighbors >= min_samples, dtype=np.uint8)
   173 1323.113 MiB    2.074 MiB       dbscan_inner(core_samples, neighborhoods, labels)
   174 1323.113 MiB    0.000 MiB       return np.where(core_samples)[0], labels


Filename: /data/home/jli819/.local/lib/python3.6/site-packages/sklearn/cluster/dbscan_.py

Line #    Mem usage    Increment   Line Contents
================================================
   298 1222.191 MiB 1222.191 MiB       @profile
   299                                 def fit(self, X, y=None, sample_weight=None):
   300                                     """Perform DBSCAN clustering from features or distance matrix.
   301                             
   302                                     Parameters
   303                                     ----------
   304                                     X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \
   305                                             array of shape (n_samples, n_samples)
   306                                         A feature array, or array of distances between samples if
   307                                         ``metric='precomputed'``.
   308                                     sample_weight : array, shape (n_samples,), optional
   309                                         Weight of each sample, such that a sample with a weight of at least
   310                                         ``min_samples`` is by itself a core sample; a sample with negative
   311                                         weight may inhibit its eps-neighbor from being core.
   312                                         Note that weights are absolute, and default to 1.
   313                             
   314                                     y : Ignored
   315                             
   316                                     """
   317 1222.191 MiB    0.000 MiB           X = check_array(X, accept_sparse='csr')
   318 1222.191 MiB    0.000 MiB           clust = dbscan(X, sample_weight=sample_weight,
   319 1232.812 MiB 1232.812 MiB                          **self.get_params())
   320 1232.812 MiB    0.000 MiB           self.core_sample_indices_, self.labels_ = clust
   321 1232.812 MiB    0.000 MiB           if len(self.core_sample_indices_):
   322                                         # fix for scipy sparse indexing issue
   323 1294.305 MiB   61.492 MiB               self.components_ = X[self.core_sample_indices_].copy()
   324                                     else:
   325                                         # no core samples
   326                                         self.components_ = np.empty((0, X.shape[1]))
   327 1294.305 MiB    0.000 MiB           return self


Filename: /data/home/jli819/Kinect2/Modules/VideoProcessor.py

Line #    Mem usage    Increment   Line Contents
================================================
   133  157.984 MiB  157.984 MiB       @profile
   134                                 def calculateHMM(self, blocksize = 5*60, delete = True):
   135                                     """
   136                                     This functon decompresses video into smaller chunks of data formated in the numpy array format.
   137                                     Each numpy array contains one row of data for the entire video.
   138                                     This function then smoothes the raw data
   139                                     Finally, an HMM is fit to the data and an HMMobject is created
   140                                     """
   141  157.984 MiB    0.000 MiB           print(self.hmmFile)
   142  157.984 MiB    0.000 MiB           print(os.path.exists(self.hmmFile))
   143  157.984 MiB    0.000 MiB           if os.path.exists(self.hmmFile) and not self.rewrite:
   144  157.984 MiB    0.000 MiB               print('Hmmfile already exists. Will not recalculate it unless rewrite flag is True')
   145  157.984 MiB    0.000 MiB               return
   146                             
   147                                     self.downloadVideo()
   148                                     
   149                                     self.blocksize = blocksize
   150                                     total_blocks = math.ceil(self.frames/(blocksize*self.frame_rate)) #Number of blocks that need to be analyzed for the full video
   151                             
   152                                     # Step 1: Convert mp4 to npy files for each row
   153                                     pool = ThreadPool(self.cores) #Create pool of threads for parallel analysis of data
   154                                     start = datetime.datetime.now()
   155                                     print('calculateHMM: Converting video into HMM data', file = sys.stderr)
   156                                     print('TotalBlocks: ' + str(total_blocks), file = sys.stderr)
   157                                     print('TotalThreads: ' + str(self.cores), file = sys.stderr)
   158                                     print('Video processed: ' + str(self.blocksize/60) + ' min per block, ' + str(self.blocksize/60*self.cores) + ' min per cycle', file = sys.stderr)
   159                                     print('Converting mp4 data to npy arrays at 1 fps', file = sys.stderr)
   160                                     print('StartTime: ' + str(start), file = sys.stderr)
   161                                     
   162                                     for i in range(0, math.ceil(total_blocks/self.cores)):
   163                                         blocks = list(range(i*self.cores, min(i*self.cores + self.cores, total_blocks)))
   164                                         print('Seconds since start: ' + str((datetime.datetime.now() - start).seconds) + ', Processing blocks: ' + str(blocks[0]) + ' to ' +  str(blocks[-1]))
   165                                         results = pool.map(self._readBlock, blocks)
   166                                         print('Data read: ' + str((datetime.datetime.now() - start).seconds) + ' seconds')
   167                                         for row in range(self.height):
   168                                             row_file = self._row_fn(row)
   169                                             out_data = np.concatenate([results[x][row] for x in range(len(results))], axis = 1)
   170                                             if os.path.isfile(row_file):
   171                                                 out_data = np.concatenate([np.load(row_file),out_data], axis = 1)
   172                                             np.save(row_file, out_data)
   173                                         print('Data wrote: ' + str((datetime.datetime.now() - start).seconds) + ' seconds', file = sys.stderr)
   174                                     pool.close() 
   175                                     pool.join() 
   176                                     print('TotalTime: ' + str((datetime.datetime.now() - start).seconds/60) + ' minutes', file = sys.stderr)
   177                             
   178                                     # Step 2: Smooth data to remove outliers
   179                                     pool = ThreadPool(self.cores)
   180                                     start = datetime.datetime.now()
   181                                     print('Smoothing data to filter out outliers', file = sys.stderr)
   182                                     print('StartTime: ' + str(start), file = sys.stderr)
   183                                     for i in range(0, self.height, self.cores):
   184                                         rows = list(range(i, min(i + self.cores, self.height)))
   185                                         print('Seconds since start: ' + str((datetime.datetime.now() - start).seconds) + ' seconds, Processing rows: ' + str(rows[0]) + ' to ' +  str(rows[-1]), file = sys.stderr)
   186                                         results = pool.map(self._smoothRow, rows)
   187                                     print('TotalTime: Took ' + str((datetime.datetime.now() - start).seconds/60) + ' minutes to smooth ' + str(self.height) + ' rows')
   188                                     pool.close() 
   189                                     pool.join()
   190                             
   191                                     # Step 3: Calculate HMM values for each row
   192                                     pool = ThreadPool(self.cores)
   193                                     start = datetime.datetime.now()
   194                                     print('Calculating HMMs for all data', file = sys.stderr)
   195                                     print('StartTime: ' + str(start), file = sys.stderr)
   196                                     for i in range(0, self.height, self.cores):
   197                                         rows = list(range(i, min(i + self.cores, self.height)))
   198                                         print('Seconds since start: ' + str((datetime.datetime.now() - start).seconds) + ' seconds, Processing rows: ' + str(rows[0]) + ' to ' +  str(rows[-1]), file = sys.stderr)
   199                                         results = pool.map(self._hmmRow, rows)
   200                                     print('TotalTime: Took ' + str((datetime.datetime.now() - start).seconds/60) + ' minutes to calculate HMMs for ' + str(self.height) + ' rows', file = sys.stderr)
   201                                     pool.close() 
   202                                     pool.join()
   203                             
   204                                     # Step 4: Create HMM object and delete temporary data if necessary
   205                                     start = datetime.datetime.now()
   206                                     if delete:
   207                                         print('Converting HMMs to internal data structure and deleting temporary data', file = sys.stderr)
   208                                     else:
   209                                         print('Converting HMMs to internal data structure and keeping temporary data', file = sys.stderr)
   210                             
   211                                     print('StartTime: ' + str(start), file = sys.stderr)
   212                                     
   213                                     self.obj = HMMdata(self.width, self.height, self.frames, self.frame_rate)
   214                                     self.obj.add_data(self.tempDirectory, self.hmmFile)
   215                                     # Copy example data to directory containing videofile
   216                                     subprocess.call(['cp', self._row_fn(int(self.height/2)), self._row_fn(int(self.height/2)).replace('.npy', '.smoothed.npy'), self._row_fn(int(self.height/2)).replace('.npy', '.hmm.npy'), self.exampleDirectory])
   217                             
   218                                     if delete:
   219                                         shutil.rmtree(self.tempDirectory)
   220                                     print('Took ' + str((datetime.datetime.now() - start).seconds/60) + ' convert HMMs', file = sys.stderr)


Filename: /data/home/jli819/Kinect2/Modules/VideoProcessor.py

Line #    Mem usage    Increment   Line Contents
================================================
   309  158.746 MiB  158.746 MiB       @profile
   310                                 def clusterHMM(self, treeR = 22, leafNum = 190, neighborR = 22, timeScale = 10, eps = 18, minPts = 170):
   311                             
   312  158.746 MiB    0.000 MiB           if os.path.exists(self.clusterDirectory + 'Labels.npy') and not self.rewrite:
   313                                         print('Cluster label file already exists. Will not recalculate it unless rewrite flag is True')
   314                                         
   315                                     else:
   316  158.746 MiB    0.000 MiB               try:
   317  158.746 MiB    0.000 MiB                   self.obj
   318  158.746 MiB    0.000 MiB               except AttributeError:
   319  830.043 MiB  671.297 MiB                   self.obj = HMMdata(filename = self.hmmFile)
   320                             
   321  830.043 MiB    0.000 MiB               print('Identifying raw coordinate positions for cluster analysis', file = sys.stderr)
   322  830.043 MiB    0.000 MiB               if os.path.isfile(self.clusterDirectory + 'FilteredCoords.npy'):
   323  830.805 MiB    0.762 MiB                   self.coords = np.load(self.clusterDirectory + 'FilteredCoords.npy')
   324  830.805 MiB    0.000 MiB                   print('self.coords size: '+str(sys.getsizeof(self.coords)))
   325  830.805 MiB    0.000 MiB                   print('self.coords count: '+str(self.coords.shape[0]))
   326                                         else:
   327                                             #self.coords = self.obj.retDBScanMatrix(minMagnitude)
   328                                             #np.save(self.clusterDirectory + 'FilteredCoords.npy', self.coords)
   329                                             print('filter HMM first')
   330                                             return 
   331                                         
   332  830.805 MiB    0.000 MiB               print('Calculating nearest neighbors and pairwise distances between clusters', file = sys.stderr)
   333                             
   334  830.805 MiB    0.000 MiB               if os.path.isfile(self.clusterDirectory + 'PairwiseDistances.npz'):
   335                                             dist = np.load(self.clusterDirectory + 'PairwiseDistances.npz')
   336                                         else:
   337  830.805 MiB    0.000 MiB                   self.coords[:,0] = self.coords[:,0].astype(np.float64) * timeScale 
   338  830.805 MiB    0.000 MiB                   if os.path.isfile(self.clusterDirectory + 'NearestNeighborTree'):
   339                                                 X = pickle.load(open(self.clusterDirectory + 'NearestNeighborTree.pkl', 'rb'))
   340                                             else:
   341  834.141 MiB    3.336 MiB                       X = NearestNeighbors(radius=treeR, metric='minkowski', p=2, algorithm='kd_tree',leaf_size=leafNum,n_jobs=24).fit(self.coords)
   342  835.906 MiB    1.766 MiB                       pickle.dump(X, open(self.clusterDirectory + 'NearestNeighborTree.pkl', 'wb'))
   343                                             
   344 1208.844 MiB  372.938 MiB                   dist = X.radius_neighbors_graph(self.coords, neighborR, 'distance')
   345 1208.844 MiB    0.000 MiB                   print('dist size: '+ str(dist.data.nbytes))
   346 1222.191 MiB 1222.191 MiB                   scipy.sparse.save_npz(self.clusterDirectory + 'PairwiseDistances.npz', dist, compressed=False)
   347                                        
   348 1232.832 MiB 1232.832 MiB               self.labels = DBSCAN(eps=eps, min_samples=minPts, metric='precomputed', n_jobs=1).fit_predict(dist)
   349                             
   350 1232.832 MiB    0.000 MiB               np.save(self.clusterDirectory + 'Labels.npy', self.labels)
   351                             
   352                                     #if os.path.exists(self.clusterDirectory + 'ClusterCenters.npy') and not self.rewrite:
   353                                     #    print('Cluster centers file already exists. Will not recalculate it unless rewrite flag is True')
   354                                     #    return
   355                             
   356 1232.832 MiB    0.000 MiB           self.coords = np.load(self.clusterDirectory + 'FilteredCoords.npy')
   357 1232.832 MiB    0.000 MiB           self.labels = np.load(self.clusterDirectory + 'Labels.npy')
   358                             
   359                                     # calculate center z, y, x, number points for each cluster
   360 1232.832 MiB    0.000 MiB           clusterDataFile = self.clusterDirectory + 'ClusterCenters.npy'
   361 1232.832 MiB    0.000 MiB           if os.path.isfile(clusterDataFile):
   362                                         self.clusterData = np.load(clusterDataFile)
   363                                     else:
   364 1232.832 MiB    0.000 MiB               uniqueLabel = set(self.labels[self.labels!=-1])
   365 1232.832 MiB    0.000 MiB               self.clusterData = np.zeros([len(uniqueLabel),7])  # clusterData z,y,x, max z,y,x, min z,y,x
   366 1232.832 MiB    0.000 MiB               for l in uniqueLabel:
   367 1232.832 MiB    0.000 MiB                   currCluster = self.coords[self.labels==l,:]
   368 1232.832 MiB    0.000 MiB                   self.clusterData[l,0:3] = np.mean(currCluster[:,0:3],axis=0)  # center coordinate of cluster
   369 1232.832 MiB    0.000 MiB                   self.clusterData[l,3] = currCluster.shape[0]  # sand change count
   370 1232.832 MiB    0.000 MiB                   self.clusterData[l,4] = np.max(currCluster[:,0]) - np.min(currCluster[:,0])
   371 1232.832 MiB    0.000 MiB                   self.clusterData[l,5] = np.max(currCluster[:,1]) - np.min(currCluster[:,1])  # span of y coordinates
   372 1232.832 MiB    0.000 MiB                   self.clusterData[l,6] = np.max(currCluster[:,2]) - np.min(currCluster[:,2]) 
   373                             
   374 1232.836 MiB    0.004 MiB               np.save(clusterDataFile, self.clusterData)
   375                             
   376                             
   377 1232.836 MiB    0.000 MiB           if os.path.exists(self.clusterDirectory + 'ClusterStat.pdf') and not self.rewrite:
   378                                         print('Cluster stats exists.')
   379                                     else:
   380 1235.363 MiB    2.527 MiB               self.clusterStat()


